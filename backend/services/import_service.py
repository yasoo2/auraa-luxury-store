# services/import_service.py
import asyncio
from typing import List, Dict, Any
from services.cj_client import list_products, get_product_details
import logging

logger = logging.getLogger(__name__)

BATCH_SIZE = 50   # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± ÙÙŠ Ø§Ù„Ø¯ÙØ¹Ø© (Ø®ÙØ¶Ù†Ø§Ù‡Ø§ Ù…Ù† 100 Ù„Ù€ 50 Ù„Ø£Ù…Ø§Ù† Ø£ÙƒØ«Ø±)
PAUSE_BETWEEN_BATCHES = 2  # Ø«ÙˆØ§Ù†ÙŠ Ø±Ø§Ø­Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¯ÙØ¹Ø§Øª

async def chunked(lst: List[Any], size: int):
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª"""
    for i in range(0, len(lst), size):
        yield lst[i:i+size]

async def bulk_import_products(
    total_count: int, 
    keyword: str = "luxury jewelry"
) -> Dict[str, Any]:
    """
    Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ù†ØªØ¬Ø§Øª Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ù…Ù† CJ Dropshipping
    
    Args:
        total_count: Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        keyword: ÙƒÙ„Ù…Ø© Ø§Ù„Ø¨Ø­Ø«
    
    Returns:
        dict Ù…Ø¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
    """
    results = {
        "total_requested": total_count,
        "total_fetched": 0,
        "ok": 0,
        "failed": 0,
        "batches": [],
        "products": []
    }

    # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    page_size = BATCH_SIZE
    num_pages = (total_count + page_size - 1) // page_size

    logger.info(f"ğŸš€ Starting bulk import: {total_count} products in {num_pages} batches")

    batch_index = 0
    products_fetched = 0
    
    for page_num in range(1, num_pages + 1):
        # Stop if we have enough products
        if products_fetched >= total_count:
            break
            
        batch_index += 1
        
        try:
            logger.info(f"ğŸ“¦ Batch {batch_index}/{num_pages}: Fetching page {page_num}")
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
            response = await list_products(
                page_num=page_num,
                page_size=page_size,
                keyword=keyword
            )
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            batch_products = []
            if response.get("result") and response["result"].get("data"):
                batch_products = response["result"]["data"]
            elif response.get("data") and isinstance(response["data"], list):
                batch_products = response["data"]
            
            # Ø­Ø¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
            remaining = total_count - products_fetched
            batch_products = batch_products[:remaining]
            
            if batch_products:
                results["products"].extend(batch_products)
                products_fetched += len(batch_products)
                results["ok"] += len(batch_products)
                
                results["batches"].append({
                    "batch": batch_index,
                    "page": page_num,
                    "size": len(batch_products),
                    "status": "success"
                })
                
                logger.info(f"âœ… Batch {batch_index} success: {len(batch_products)} products ({products_fetched}/{total_count})")
            else:
                logger.warning(f"âš ï¸ Batch {batch_index}: No products returned")
                results["batches"].append({
                    "batch": batch_index,
                    "page": page_num,
                    "size": 0,
                    "status": "empty"
                })
                break  # No more products available
            
        except Exception as e:
            logger.error(f"âŒ Batch {batch_index} failed: {e}")
            results["failed"] += page_size
            results["batches"].append({
                "batch": batch_index,
                "page": page_num,
                "size": 0,
                "status": f"error: {str(e)[:100]}"
            })
        
        # Ø±Ø§Ø­Ø© Ù‚ØµÙŠØ±Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¯ÙØ¹Ø§Øª Ù„ØªÙØ§Ø¯ÙŠ 429
        if batch_index < num_pages:
            logger.info(f"ğŸ˜´ Sleeping {PAUSE_BETWEEN_BATCHES}s before next batch...")
            await asyncio.sleep(PAUSE_BETWEEN_BATCHES)
    
    results["total_fetched"] = products_fetched
    
    logger.info(f"âœ… Bulk import complete: {products_fetched}/{total_count} products fetched")
    
    return results

async def fetch_product_details_batch(product_ids: List[str]) -> List[Dict[str, Any]]:
    """
    Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ù…Ù†ØªØ¬Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
    
    Args:
        product_ids: Ù‚Ø§Ø¦Ù…Ø© IDs Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
    
    Returns:
        Ù‚Ø§Ø¦Ù…Ø© ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
    """
    results = []
    batch_index = 0
    
    async for batch in chunked(product_ids, BATCH_SIZE):
        batch_index += 1
        logger.info(f"ğŸ“¦ Fetching details batch {batch_index}: {len(batch)} products")
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ (Ù…Ø­Ù…ÙŠ Ø¨Ù€ semaphore)
            tasks = [get_product_details(pid) for pid in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ Failed to fetch {batch[i]}: {result}")
                else:
                    results.append(result)
            
            logger.info(f"âœ… Details batch {batch_index} complete")
            
        except Exception as e:
            logger.error(f"âŒ Details batch {batch_index} failed: {e}")
        
        # Ø±Ø§Ø­Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¯ÙØ¹Ø§Øª
        await asyncio.sleep(PAUSE_BETWEEN_BATCHES)
    
    return results
